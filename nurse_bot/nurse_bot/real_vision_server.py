import os
import rclpy
from rclpy.node import Node
import cv2
import numpy as np
import mediapipe as mp
import colorsys
from mediapipe.tasks import python
from mediapipe.tasks.python import vision

# ROS2 ë©”ì‹œì§€ ë° ì„œë¹„ìŠ¤
from sensor_msgs.msg import Image, CameraInfo
from cv_bridge import CvBridge
from ament_index_python.packages import get_package_share_directory
from ultralytics import YOLO
from od_msg.srv import SrvDepthPosition  # ì„œë¹„ìŠ¤ ë©”ì‹œì§€ íƒ€ì…

class ImgNode(Node):
    def __init__(self):
        super().__init__('img_node')
        self.bridge = CvBridge()
        self.color_frame = None
        self.depth_frame = None
        self.intrinsics = None

        # RealSense í† í”½ êµ¬ë…
        self.create_subscription(Image, '/camera/camera/color/image_raw', self.color_callback, 10)
        self.create_subscription(Image, '/camera/camera/aligned_depth_to_color/image_raw', self.depth_callback, 10)
        self.create_subscription(CameraInfo, '/camera/camera/color/camera_info', self.camera_info_callback, 10)

    def camera_info_callback(self, msg):
        self.intrinsics = {"fx": msg.k[0], "fy": msg.k[4], "ppx": msg.k[2], "ppy": msg.k[5]}

    def color_callback(self, msg):
        try:
            self.color_frame = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')
        except Exception as e:
            self.get_logger().error(f"Color frame error: {e}")

    def depth_callback(self, msg):
        try:
            self.depth_frame = self.bridge.imgmsg_to_cv2(msg, desired_encoding='passthrough')
        except Exception as e:
            self.get_logger().error(f"Depth frame error: {e}")

class RealVisionServer(ImgNode):
    def __init__(self):
        super().__init__()
        
        # ===============================
        # 1. ëª¨ë¸ ë¡œë“œ
        # ===============================
        pkg_path = get_package_share_directory("nurse_bot")
        tool_model_path = os.path.join(pkg_path, "resource", "tool_best.pt")
        hand_model_path = os.path.join(pkg_path, "resource", "hand_landmarker.task")

        self.get_logger().info("Loading Models...")
        
        # YOLO (ë„êµ¬ìš©)
        try:
            self.model_tool = YOLO(tool_model_path)
            self.get_logger().info(f"ğŸ”§ Tool Model Loaded")
        except Exception as e:
            self.get_logger().error(f"Failed to load Tool Model: {e}")
            self.model_tool = None

        # MediaPipe (.task íŒŒì¼ìš©)
        try:
            base_options = python.BaseOptions(model_asset_path=hand_model_path)
            options = vision.HandLandmarkerOptions(
                base_options=base_options,
                num_hands=1,
                min_hand_detection_confidence=0.6,
                min_hand_presence_confidence=0.6,
                min_tracking_confidence=0.5,
                # ì„œë¹„ìŠ¤ ìš”ì²­ ì‹œ 1ì¥ë§Œ ì²˜ë¦¬í•˜ë¯€ë¡œ IMAGE ëª¨ë“œ ì‚¬ìš©
                running_mode=vision.RunningMode.IMAGE 
            )
            self.landmarker = vision.HandLandmarker.create_from_options(options)
            self.get_logger().info("ğŸ‘‰ MediaPipe HandLandmarker (.task) Loaded")
        except Exception as e:
            self.get_logger().error(f"Failed to load Hand Task: {e}")
            self.landmarker = None

        # ì‹œê°í™”ìš© ì„¤ì • (ë¬´ì§€ê°œìƒ‰ ê´€ì ˆ)
        self.HAND_CONNECTIONS = [
	    (0, 1), (1, 2), (2, 3), (3, 4), (0, 5), (5, 6), (6, 7), (7, 8),
	    (0, 9), (9, 10), (10, 11), (11, 12), (0, 13), (13, 14), (14, 15), (15, 16),
	    (0, 17), (17, 18), (18, 19), (19, 20)
	]
        self.landmark_colors = []
        for i in range(21):
            r, g, b = colorsys.hsv_to_rgb(i / 21.0, 1.0, 1.0)
            self.landmark_colors.append((int(b * 255), int(g * 255), int(r * 255)))

        # ë°ì´í„° ê³µìœ  ë³€ìˆ˜ (ë§ˆì§€ë§‰ ê°ì§€ ê²°ê³¼ ì €ì¥ìš©)
        self.detected_point = None      
        self.detected_label = ""        
        self.latest_hand_landmarks = [] 
        self.latest_tool_kpts = []  
        self.detected_angle = 0.0
        self.latest_bbox = None    

        # ===============================
        # 2. ì„œë¹„ìŠ¤ ì„œë²„ ìƒì„±
        # ===============================
        self.srv = self.create_service(SrvDepthPosition, '/get_3d_position', self.detect_callback)
        
        # í™”ë©´ ê°±ì‹ ìš© íƒ€ì´ë¨¸ (ì¸ì‹ ë¡œì§ ì—†ìŒ, ë‹¨ìˆœíˆ ê·¸ë¦¼ë§Œ ê·¸ë¦¼)
        self.create_timer(0.1, self.display_timer_callback)
        
        self.get_logger().info("ğŸ‘€ Real Vision Service Ready! Waiting for Request...")

    def detect_callback(self, request, response):
        """ì„œë¹„ìŠ¤ ìš”ì²­ì´ ë“¤ì–´ì™”ì„ ë•Œë§Œ ì‹¤í–‰ë˜ëŠ” í•¨ìˆ˜"""
        target = request.target
        self.get_logger().info(f"ğŸ“¥ Request Received: Find '{target}'")

        if self.color_frame is None or self.depth_frame is None or self.intrinsics is None:
            self.get_logger().warn("âš ï¸ Camera frames not ready yet.")
            return response

        # ì´ˆê¸°í™” (ìƒˆë¡œìš´ ìš”ì²­ì´ ì™”ìœ¼ë¯€ë¡œ ì´ì „ ê²°ê³¼ ì´ˆê¸°í™”)
        u, v = 0, 0
        z_mm = 0.0
        found = False
        self.detected_point = None
        self.latest_hand_landmarks = []
        self.latest_tool_kpts = []
        self.detected_label = ""
        self.detected_angle = 0.0
        self.latest_bbox = None
         # â˜…â˜…â˜… [ì¶”ê°€ëœ ê¸°ëŠ¥] ê±°ë¦¬ ì¸¡ì • ëª¨ë“œ (measure) â˜…â˜…â˜…
        if target == "measure":
            h, w = self.depth_frame.shape
            u, v = w // 2, h // 3 # í™”ë©´ ì •ì¤‘ì•™ ì¢Œí‘œ
            
            # ì¤‘ì•™ í”½ì…€ì˜ ê¹Šì´ê°’ ê°€ì ¸ì˜¤ê¸° (mm ë‹¨ìœ„)
            z_mm = float(self.depth_frame[v, u])
            
            if z_mm > 0:
                self.get_logger().info(f"ğŸ“ ë°”ë‹¥ê¹Œì§€ ê±°ë¦¬ ì¸¡ì •ë¨: {z_mm}mm")
                # X, YëŠ” 0ìœ¼ë¡œ ë³´ë‚´ê³  Z(ê¹Šì´)ë§Œ ì±„ì›Œì„œ ë³´ëƒ„
                response.depth_position = [0.0, 0.0, z_mm]
                return response
            else:
                self.get_logger().warn("âš ï¸ ê±°ë¦¬ ì¸¡ì • ì‹¤íŒ¨ (ë„ˆë¬´ ê°€ê¹ê±°ë‚˜ ë©‰ë‹ˆë‹¤)")
                return response
        # ==========================================
        # CASE 1: ì† ì¸ì‹ (MediaPipe)
        # ==========================================
        if target.lower() in ['hand']:
            if self.landmarker:
                img_rgb = cv2.cvtColor(self.color_frame, cv2.COLOR_BGR2RGB)
                mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=img_rgb)
                
                # ì¶”ë¡  ì‹¤í–‰
                detection_result = self.landmarker.detect(mp_image)

                if detection_result.hand_landmarks:
                    hand_landmarks = detection_result.hand_landmarks[0]
                    self.latest_hand_landmarks = hand_landmarks # ì‹œê°í™”ìš© ë°ì´í„° ì—…ë°ì´íŠ¸
                    self.detected_label = "Hand"
                    
                    h, w, _ = self.color_frame.shape
                    # Wrist(0), Index(5), Pinky(17) Center
                    idx_points = [0, 5, 17]
                    sum_x, sum_y = 0, 0
                    for idx in idx_points:
                        lm = hand_landmarks[idx]
                        sum_x += int(lm.x * w)
                        sum_y += int(lm.y * h)
                    
                    u = sum_x // 3
                    v = sum_y // 3
                    found = True
                else:
                    self.get_logger().info("âŒ Hand not found")

        # ==========================================
        # CASE 2: ë„êµ¬ ì¸ì‹ (YOLO Pose)
        # ==========================================
        else:
            if self.model_tool:
                results = self.model_tool(self.color_frame, conf=0.5, iou=0.75, classes=[0,1,2,3], verbose=False)
                best_conf = 0.0
                best_center = None
                
                if results[0].boxes and results[0].keypoints is not None:
                    boxes = results[0].boxes
                    keypoints = results[0].keypoints.xy.cpu().numpy()

                    for i, box in enumerate(boxes):
                        cls_name = self.model_tool.names[int(box.cls[0])]
                        # ìš”ì²­í•œ target ì´ë¦„ì´ í¬í•¨ë˜ëŠ”ì§€ í™•ì¸
                        if target.lower() in cls_name.lower():
                            kpts = keypoints[i]
                            # í‚¤í¬ì¸íŠ¸ 2ê°œ ì´ìƒì¼ ë•Œë§Œ (ì–‘ ëì  ê°€ì •)
                            if len(kpts) >= 2 and box.conf[0] > best_conf:
                                x1, y1 = map(int, kpts[0])
                                x2, y2 = map(int, kpts[1])
                                
                                self.latest_tool_kpts = [(x1, y1), (x2, y2)]
                                self.detected_label = f"Tool: {target}"
                                
                                cx, cy = (x1 + x2) // 2, (y1 + y2) // 2
                                best_center = (cx, cy)
                                best_conf = float(box.conf[0])

                                bx1, by1, bx2, by2 = map(int, box.xyxy[0])
                                self.latest_bbox = (bx1, by1, bx2, by2)
                
                if best_center:
                    u, v = best_center
                    found = True
                else:
                    self.get_logger().info(f"âŒ Tool '{target}' not found")

        # ==========================================
        # 3D ì¢Œí‘œ ë³€í™˜ ë° ì‘ë‹µ ë°˜í™˜
        # ==========================================
        if found:
            # Depth ì²´í¬
            if 0 <= u < self.depth_frame.shape[1] and 0 <= v < self.depth_frame.shape[0]:
                z_mm = float(self.depth_frame[v, u])
            else:
                z_mm = 0.0

            if z_mm > 0:
                fx, fy = self.intrinsics['fx'], self.intrinsics['fy']
                cx, cy = self.intrinsics['ppx'], self.intrinsics['ppy']

                x_val = (u - cx) * z_mm / fx
                y_val = (v - cy) * z_mm / fy

                self.get_logger().info(f"ğŸ¯ Success [{target}]: ({x_val:.1f}, {y_val:.1f}, {z_mm:.1f})")
                
                # ì„œë¹„ìŠ¤ ì‘ë‹µì— ì¢Œí‘œ ì…ë ¥
                response.depth_position = [x_val, y_val, z_mm]
                
                # ì‹œê°í™”ìš© ì¢Œí‘œ ì—…ë°ì´íŠ¸
                self.detected_point = (u, v)
            else:
                self.get_logger().warn("âš ï¸ Detected but Depth is 0")
        
        return response

    def display_timer_callback(self):
        """ì¸ì‹ ê²°ê³¼ ì‹œê°í™” (ì¸ì‹ ë¡œì§ì€ ì—†ìŒ)"""
        if self.color_frame is None: return

        display_img = self.color_frame.copy()
        h, w, _ = display_img.shape

        # 1. ì† ëœë“œë§ˆí¬ ê·¸ë¦¬ê¸°
        if self.latest_hand_landmarks:
            for start_idx, end_idx in self.HAND_CONNECTIONS:
                p1 = self.latest_hand_landmarks[start_idx]
                p2 = self.latest_hand_landmarks[end_idx]
                cv2.line(display_img, (int(p1.x*w), int(p1.y*h)), (int(p2.x*w), int(p2.y*h)), (200, 200, 200), 2)
            for i, lm in enumerate(self.latest_hand_landmarks):
                cv2.circle(display_img, (int(lm.x*w), int(lm.y*h)), 5, self.landmark_colors[i], -1)

        # 2. ë„êµ¬ í‚¤í¬ì¸íŠ¸ ê·¸ë¦¬ê¸°
        for (tx, ty) in self.latest_tool_kpts:
            cv2.circle(display_img, (tx, ty), 5, (0, 255, 255), -1)


        # 3. íƒ€ê²Ÿ ì¢Œí‘œ ë° ë¼ë²¨ ê·¸ë¦¬ê¸°
        if self.detected_point:
            u, v = self.detected_point
            cv2.circle(display_img, (u, v), 8, (0, 255, 0), -1)
            cv2.circle(display_img, (u, v), 12, (0, 255, 0), 2)
            cv2.putText(display_img, self.detected_label, (u + 15, v - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
        
        cv2.imshow("Nurse Bot Service View", display_img)
        cv2.waitKey(1)

def main():
    rclpy.init()
    node = RealVisionServer()
    rclpy.spin(node)
    rclpy.shutdown()

if __name__ == '__main__':
    main()
